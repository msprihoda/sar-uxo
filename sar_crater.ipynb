{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:39.713590399Z",
     "start_time": "2023-11-25T21:51:38.419511379Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports here are through Anaconda (Conda) for the primary PyTorch, Lightning, and TorchMetrics\n",
    "# libraries. PIL, and NumPy are also used. Matplotlib is extraneous\n",
    "\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, BasePredictionWriter\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "import lightning as L\n",
    "import torchmetrics.classification as TM\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d4391663882220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:40.127093392Z",
     "start_time": "2023-11-25T21:51:40.124030605Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# I set this to \"high\" precision for much faster compute time, it's possible for better\n",
    "# performance not using this, but it does result in about a 3-4 times faster compute\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69810a98b57bf0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:41.714073158Z",
     "start_time": "2023-11-25T21:51:41.711409486Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the core double convolution of U-Net. It is reused in the up and down\n",
    "# parts of the U-Net architecture. The Mish function is used as I found it works\n",
    "# best, but ReLU, SiLU, etc. also work. Mish was what I found works best personally.\n",
    "\n",
    "# We have: [2D convolution, batch normalization (instance works less well), Mish] x2\n",
    "class DubConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.dub_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.Mish(inplace=True)   \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dub_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15012a40c917eb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:42.848090431Z",
     "start_time": "2023-11-25T21:51:42.843139950Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Downward limb of our U-Net. I had read studies which suggest the Average Pooling\n",
    "# performs better with SAR imagery than Max Pooling. I found this to be the case as\n",
    "# well and used this here.\n",
    "\n",
    "# We have an [Average Pooling, Double Convolution] Here.\n",
    "class Down(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.avgpool_conv = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            DubConv(in_ch, out_ch)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.avgpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44829b20b4502e40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:43.623916406Z",
     "start_time": "2023-11-25T21:51:43.622046070Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Upward Limb of our U-Net. We upsample using nearest neighbor algorithm. I haven't\n",
    "# tested with this to find it is the best, however it was the algorithm use by my advisor\n",
    "# and I remained with it. Bilinear *may* work better, but I cannot speak to this.\n",
    "\n",
    "# We have [UpSample, Double Convolution]\n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(in_ch, in_ch // 2, kernel_size=1)\n",
    "        )\n",
    "        self.conv = DubConv(in_ch, out_ch)\n",
    "\n",
    "# Here we have padding to \"restore\" or maintain the input image size as well.\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        # Pad x1 to the size of x2\n",
    "        diff_h = x2.shape[2] - x1.shape[2]\n",
    "        diff_w = x2.shape[3] - x1.shape[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2])\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cb4fad6084dd780",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:44.332761854Z",
     "start_time": "2023-11-25T21:51:44.330717964Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Here is our core U-Net architecture. I generally used \"default\" or established values\n",
    "# for the number of layers and features as found in other works. This works well, though\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int = 1, num_layers: int = 5, features_start: int = 64):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [DubConv(1, features_start)]\n",
    "\n",
    "        feats = features_start\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Down(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2))\n",
    "            feats //= 2\n",
    "# We append the previous convolution layers to the new layer, as to U-Net architecture\n",
    "        \n",
    "        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xi = [self.layers[0](x)]\n",
    "\n",
    "        for layer in self.layers[1: self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "\n",
    "        for i, layer in enumerate(self.layers[self.num_layers: -1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "            \n",
    "        logits = self.layers[-1](xi[-1])\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14945f18f6f8d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:45.440063397Z",
     "start_time": "2023-11-25T21:51:45.438129276Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Transformations applied to input data, to enrich training data\n",
    "\n",
    "transform = v2.RandomApply(transforms=[\n",
    "    v2.RandomAffine((-90, 90)),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=3),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomVerticalFlip(),\n",
    "    v2.RandomRotation(90)],\n",
    "    p=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48500a6704185a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:46.124173224Z",
     "start_time": "2023-11-25T21:51:46.122227544Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch Dataset Path (Adjust as needed to your directories)\n",
    "class SARTrainData(Dataset):\n",
    "    IMAGE_PATH = \"images\"\n",
    "    MASK_PATH = \"labels\"\n",
    "    data_path = \"/home/michael/sar_crater/train_Sig0db\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str,\n",
    "            img_size: tuple = (256, 256),\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.img_path = os.path.join(self.data_path, self.IMAGE_PATH)\n",
    "        self.mask_path = os.path.join(self.data_path, self.MASK_PATH)\n",
    "        self.img_list = self.get_filenames(self.img_path)\n",
    "        self.mask_list = self.get_filenames(self.mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        process = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32),\n",
    "            v2.Normalize((0.5, ), (0.5, ))\n",
    "        ])\n",
    "        \n",
    "        image = Image.open(self.img_list[idx])\n",
    "        mask = Image.open(self.mask_list[idx])\n",
    "        #image = np.asarray([image], dtype=np.float32)\n",
    "        mask = np.asarray([mask], dtype=np.float32)\n",
    "        image = process(image)\n",
    "        mask = torch.from_numpy(mask)\n",
    "        \n",
    "        img, mask = self.transform(image, mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "    def get_filenames(self, path):\n",
    "        files_list = []\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".tif\"):\n",
    "                    files_list.append(os.path.join(path, filename))\n",
    "        return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50c10e0b887d5b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:46.932555510Z",
     "start_time": "2023-11-25T21:51:46.930237944Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Prediction dataset directory\n",
    "class PredData(Dataset):\n",
    "\n",
    "    data_path = \"/home/michael/sar_crater/pred/pred_Sig0db\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str,\n",
    "    ):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.img_path = self.data_path\n",
    "        self.img_list = self.get_filenames(self.img_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        process = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize((0.5, ), (0.5, ))\n",
    "        ])\n",
    "\n",
    "        image = Image.open(self.img_list[idx])\n",
    "        img = process([image])\n",
    "\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def get_filenames(self, path):\n",
    "        files_list = []\n",
    "        for filename in os.listdir(path):\n",
    "            files_list.append(os.path.join(path, filename))\n",
    "        return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "226465f993f134a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:47.599656755Z",
     "start_time": "2023-11-25T21:51:47.597320838Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Lightning Datamodule\n",
    "class SARDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"/home/michael/sar_crater/train_Sig0db\", batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            SAR_full = SARTrainData(self.data_dir)\n",
    "            self.SAR_train, self.SAR_val = random_split(\n",
    "                SAR_full, [0.8, 0.2], generator=torch.Generator().manual_seed(37)\n",
    "            )\n",
    "            \n",
    "        if stage == \"predict\":\n",
    "            self.PredData = PredData(\"/home/michael/sar_crater/pred_Sig0db\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.SAR_train, batch_size=self.batch_size, num_workers=18, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.SAR_val, batch_size=self.batch_size, num_workers=8, persistent_workers=True)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.PredData, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a7e9be10a6fc2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T00:06:52.878457985Z",
     "start_time": "2023-11-26T00:06:52.833786412Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Lightning module combining the previous elements, makes it easier to make modifications in specific values\n",
    "class LitUNet(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str = \"/home/michael/sar_crater/train_Sig0db\",\n",
    "            batch_size: int = 16,\n",
    "            lr: float = 0.0137,\n",
    "            num_layers: int = 5,\n",
    "            features_start: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_layers = num_layers\n",
    "        self.features_start = features_start\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.net = UNet()\n",
    "        \n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([20]))\n",
    "\n",
    "        self.t_acc = TM.BinaryAccuracy(ignore_index=0)\n",
    "        self.v_acc = TM.BinaryAccuracy(ignore_index=0)\n",
    "        self.v_recall = TM.BinaryRecall()\n",
    "        self.v_precision = TM.BinaryPrecision()\n",
    "        self.v_f1 = TM.BinaryF1Score()\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.net(batch)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        out = self(img)\n",
    "        loss = self.loss_fn(out, mask)\n",
    "        self.t_acc(out, mask.short())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.log(\"tacc\", self.t_acc, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        out = self(img)\n",
    "        val_loss = self.loss_fn(out, mask)\n",
    "        self.v_acc(out, mask.short())\n",
    "        self.v_recall(out, mask.short())\n",
    "        self.v_precision(out, mask.short())\n",
    "        self.v_f1(out, mask.short())\n",
    "        self.log(\"val_loss\", val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"vacc\", self.v_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"vrecall\", self.v_recall, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"vprec\", self.v_precision, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"vf1\", self.v_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        img, mask = batch\n",
    "        out = self.net(img)\n",
    "        recall = self.recall(out, mask.short())\n",
    "        precision = self.precision(out, mask.short())\n",
    "        f1 = self.f1(out, mask.short())\n",
    "        log_dict = {\"recall\": recall, \"precision\": precision, \"f1\": f1}\n",
    "\n",
    "        self.log_dict(log_dict, logger=True, on_step=True)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        preds = self(batch)\n",
    "        return preds\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.SGD(self.net.parameters(), lr=self.lr)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=24)\n",
    "\n",
    "        return [opt], [sch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71835ef276e21ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T00:00:31.898687977Z",
     "start_time": "2023-11-26T00:00:31.854615914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prediction writer class, makes output images\n",
    "class PredWriter(BasePredictionWriter):\n",
    "    \n",
    "    def __init__(self, output_dir, write_interval):\n",
    "        super().__init__(write_interval)\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):\n",
    "        torch.save(predictions, os.path.join(self.output_dir, \"predictions.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4ae867fba77f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T21:51:50.078266929Z",
     "start_time": "2023-11-25T21:51:49.523474004Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Lightning module command center\n",
    "\n",
    "model = LitUNet()\n",
    "mlf_logger = MLFlowLogger(\n",
    "    experiment_name=\"litUnetLogs\", \n",
    "    tracking_uri=\"http://localhost:3737\",\n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"vf1\",\n",
    "    dirpath=\"/home/michael/sar_crater/ckpt\",\n",
    "    filename=\"sar-{epoch:02d}-{vf1:.2f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "pred_writer = PredWriter(output_dir=\"/home/michael/sar_crater/inference\", write_interval=\"epoch\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\", devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    profiler=\"simple\",\n",
    "    precision=\"16-mixed\",\n",
    "    default_root_dir=\"/home/michael/sar_crater\",\n",
    "    enable_checkpointing=True,\n",
    "    logger=mlf_logger,\n",
    "    max_epochs=1000,\n",
    "    callbacks=[checkpoint_callback, pred_writer]\n",
    ")\n",
    "\n",
    "dm = SARDataModule(data_dir=\"/home/michael/sar_crater/train_Sig0db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d5046ae253b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T22:48:08.776776898Z",
     "start_time": "2023-11-25T21:51:50.671079182Z"
    },
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/sar_crater/py_env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/michael/sar_crater/ckpt exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | net         | UNet              | 28.9 M | train\n",
      "1 | loss_fn     | BCEWithLogitsLoss | 0      | train\n",
      "2 | t_acc       | BinaryAccuracy    | 0      | train\n",
      "3 | v_acc       | BinaryAccuracy    | 0      | train\n",
      "4 | v_recall    | BinaryRecall      | 0      | train\n",
      "5 | v_precision | BinaryPrecision   | 0      | train\n",
      "6 | v_f1        | BinaryF1Score     | 0      | train\n",
      "----------------------------------------------------------\n",
      "28.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.9 M    Total params\n",
      "115.790   Total estimated model params size (MB)\n",
      "109       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 236/236 [00:42<00:00,  5.50it/s, v_num=1e9c, train_loss_step=0.425, tacc_step=0.324]\n",
      "\u001b[Aidation: |          | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|          | 0/59 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|          | 0/59 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   2%|▏         | 1/59 [00:00<00:03, 15.42it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▎         | 2/59 [00:00<00:03, 15.31it/s]\n",
      "\u001b[Aidation DataLoader 0:   5%|▌         | 3/59 [00:00<00:03, 15.33it/s]\n",
      "\u001b[Aidation DataLoader 0:   7%|▋         | 4/59 [00:00<00:03, 15.28it/s]\n",
      "\u001b[Aidation DataLoader 0:   8%|▊         | 5/59 [00:00<00:03, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█         | 6/59 [00:00<00:03, 15.28it/s]\n",
      "\u001b[Aidation DataLoader 0:  12%|█▏        | 7/59 [00:00<00:03, 15.31it/s]\n",
      "\u001b[Aidation DataLoader 0:  14%|█▎        | 8/59 [00:00<00:03, 15.33it/s]\n",
      "\u001b[Aidation DataLoader 0:  15%|█▌        | 9/59 [00:00<00:03, 15.29it/s]\n",
      "\u001b[Aidation DataLoader 0:  17%|█▋        | 10/59 [00:00<00:03, 15.31it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|█▊        | 11/59 [00:00<00:03, 15.30it/s]\n",
      "\u001b[Aidation DataLoader 0:  20%|██        | 12/59 [00:00<00:03, 15.30it/s]\n",
      "\u001b[Aidation DataLoader 0:  22%|██▏       | 13/59 [00:00<00:03, 15.29it/s]\n",
      "\u001b[Aidation DataLoader 0:  24%|██▎       | 14/59 [00:00<00:02, 15.30it/s]\n",
      "\u001b[Aidation DataLoader 0:  25%|██▌       | 15/59 [00:00<00:02, 15.31it/s]\n",
      "\u001b[Aidation DataLoader 0:  27%|██▋       | 16/59 [00:01<00:02, 15.32it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|██▉       | 17/59 [00:01<00:02, 15.32it/s]\n",
      "\u001b[Aidation DataLoader 0:  31%|███       | 18/59 [00:01<00:02, 15.30it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|███▏      | 19/59 [00:01<00:02, 15.30it/s]\n",
      "\u001b[Aidation DataLoader 0:  34%|███▍      | 20/59 [00:01<00:02, 15.29it/s]\n",
      "\u001b[Aidation DataLoader 0:  36%|███▌      | 21/59 [00:01<00:02, 15.28it/s]\n",
      "\u001b[Aidation DataLoader 0:  37%|███▋      | 22/59 [00:01<00:02, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|███▉      | 23/59 [00:01<00:02, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  41%|████      | 24/59 [00:01<00:02, 15.28it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|████▏     | 25/59 [00:01<00:02, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  44%|████▍     | 26/59 [00:01<00:02, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  46%|████▌     | 27/59 [00:01<00:02, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  47%|████▋     | 28/59 [00:01<00:02, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  49%|████▉     | 29/59 [00:01<00:01, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  51%|█████     | 30/59 [00:01<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  53%|█████▎    | 31/59 [00:02<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  54%|█████▍    | 32/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  56%|█████▌    | 33/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████▊    | 34/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  59%|█████▉    | 35/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████    | 36/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  63%|██████▎   | 37/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  64%|██████▍   | 38/59 [00:02<00:01, 15.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  66%|██████▌   | 39/59 [00:02<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|██████▊   | 40/59 [00:02<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  69%|██████▉   | 41/59 [00:02<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|███████   | 42/59 [00:02<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  73%|███████▎  | 43/59 [00:02<00:01, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  75%|███████▍  | 44/59 [00:02<00:00, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  76%|███████▋  | 45/59 [00:02<00:00, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  78%|███████▊  | 46/59 [00:03<00:00, 15.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  80%|███████▉  | 47/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|████████▏ | 48/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  83%|████████▎ | 49/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  85%|████████▍ | 50/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  86%|████████▋ | 51/59 [00:03<00:00, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  88%|████████▊ | 52/59 [00:03<00:00, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|████████▉ | 53/59 [00:03<00:00, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  92%|█████████▏| 54/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  93%|█████████▎| 55/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  95%|█████████▍| 56/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|█████████▋| 57/59 [00:03<00:00, 15.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  98%|█████████▊| 58/59 [00:03<00:00, 15.26it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|██████████| 59/59 [00:03<00:00, 15.28it/s]\n",
      "Epoch 1:  22%|██▏       | 52/236 [00:09<00:33,  5.46it/s, v_num=1e9c, train_loss_step=0.297, tacc_step=0.468, val_loss=0.271, vacc=0.548, vrecall=0.548, vprec=0.306, vf1=0.393, train_loss_epoch=0.420, tacc_epoch=0.540] "
     ]
    }
   ],
   "source": [
    "# Run module for fit\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ede0e71376f205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T00:07:01.130125102Z",
     "start_time": "2023-11-26T00:07:00.652522968Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run module to predict\n",
    "trainer.predict(datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
